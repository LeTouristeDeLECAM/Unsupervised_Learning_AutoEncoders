# This library come from the book "Inside deep learning" by Edward Raff 
# ISBN : 978-1661729-863-9 
# GitHub : https://github.com/EdwardRaff/Inside-Deep-Learning/blob/main/idlmam.py

import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader

from tqdm.autonotebook import tqdm #Progress bar


import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import pandas as pd

import time

#---------------------------------
def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix="", desc=None):

        
        
    """
    model -- the PyTorch model / "Module" to run for one epoch
    optimizer -- the object that will update the weights of the network
    data_loader -- DataLoader object that returns tuples of (input, label) pairs. 
    loss_func -- the loss function that takes in two arguments, the model outputs and the labels, and returns a score
    device -- the compute lodation to perform training
    score_funcs -- a dictionary of scoring functions to use to evalue the performance of the model
    prefix -- a string to pre-fix to any scores placed into the _results_ dictionary. 
    desc -- a description to use for the progress bar.     
    """
       
    running_loss = []
    y_true = []
    y_pred = []
    start = time.time()

    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):
        #Move the batch to the device we are using. 
        inputs = moveTo(inputs, device)
        labels = moveTo(labels, device)

        
        y_hat = model(inputs) #this just computed f_Î˜(x(i))
        # Compute loss.
        loss = loss_func(y_hat, labels)
        
        if model.training:
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        #Now we are just grabbing some information we would like to have
        running_loss.append(loss.item())

        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):
            #moving labels & predictions back to CPU for computing / storing predictions
            labels = labels.detach().cpu().numpy()
            y_hat = y_hat.detach().cpu().numpy()
            #add to predictions so far
            y_true.extend(labels.tolist())
            y_pred.extend(y_hat.tolist())
    #end training epoch
    end = time.time()

    y_pred = np.asarray(y_pred) #Convert to numpy array
    if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: #We have a classification problem, convert to labels
        y_pred = np.argmax(y_pred, axis=1)
    #Else, we assume we are working on a regression problem
    
    results[prefix + " loss"].append( np.mean(running_loss) )
    for name, score_func in score_funcs.items():
        try:
            results[prefix + " " + name].append( score_func(y_true, y_pred) )
        except:
            results[prefix + " " + name].append(float("NaN"))
    return end-start #time spent on epoch


#---------------------------------
def moveTo(obj, device):
    """
    obj: the python object to move to a device, or to move its contents to a device
    device: the compute device to move objects to
    """
    if hasattr(obj, "to"):
        return obj.to(device)
    elif isinstance(obj, list):
        return [moveTo(x, device) for x in obj]
    elif isinstance(obj, tuple):
        return tuple(moveTo(list(obj), device))
    elif isinstance(obj, set):
        return set(moveTo(list(obj), device))
    elif isinstance(obj, dict):
        to_ret = dict()
        for key, value in obj.items():
            to_ret[moveTo(key, device)] = moveTo(value, device)
        return to_ret
    else:
        return obj
        
#---------------------------------


def train_network (model, loss_func, train_loader, val_loader=None, test_loader=None, score_funcs=None, epochs=50, device= "cpu", checkpoint_file = None, lr_schedule= None, optimizer= None, disable_tqdm=False):
    """
    Train a neural network.
    model: the model to train
    loss_func: the loss function
    train_loader: the training data
    val_loader: the validation data
    test_loader: optional test data to evaluate on after every epoch
    score_funcs: a dictionary of functions to compute the scores
    epoch: the number of epochs
    device: the device to use
    checkpoint_file: the file to save the model
    lr_schedule: the learning rate scheduler
    optimizer: the optimizer
    disable_tqdm: disable the progress bar
    """

    if score_funcs == None:
        score_funcs = {}#Empty set 

    to_track = ["epoch", "total time", "train loss"]

    if val_loader is not None:
        to_track.append("val loss")
    if test_loader is not None:
        to_track.append("test loss")
    for eval_score in score_funcs:
        to_track.append("train " + eval_score )
        if val_loader is not None:
            to_track.append("val " + eval_score )
        if test_loader is not None:
            to_track.append("test "+ eval_score )
    
    total_train_time = 0 #Total time of training
    results = {}

    # Initialize item with an empty list 

    for item in to_track:
        results[item] = []

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters()) #Adam optimizer by default
        del_opt = True
    else:
        del_opt = False

    #Place the model on the correct compute resource (CPU or GPU)
    model.to(device)


    for epoch in tqdm(range(epochs), desc="Epoch", disable=disable_tqdm):

        model = model.train() #Set the model to train mode

        total_train_time += run_epoch (model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix="train", desc="Training")
        results["epoch"].append( epoch )
        results["total time"].append( total_train_time )



        if val_loader is not None:
            model = model.eval() #Set the model to "evaluation" mode, b/c we don't want to make any updates!
            with torch.no_grad():
                run_epoch(model, optimizer, val_loader, loss_func, device, results, score_funcs, prefix="val", desc="Validating")
                


        #In PyTorch, the convention is to update the learning rate after every epoch
        if lr_schedule is not None:
            if isinstance(lr_schedule, torch.optim.lr_scheduler.ReduceLROnPlateau):
                lr_schedule.step(results["val loss"][-1])
            else:
                lr_schedule.step()


        if test_loader is not None:
            model = model.eval() #Set the model to "evaluation" mode, b/c we don't want to make any updates!
            with torch.no_grad():
                run_epoch(model, optimizer, test_loader, loss_func, device, results, score_funcs, prefix="test", desc="Testing")
        
       
        if checkpoint_file is not None:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'results' : results
                }, checkpoint_file)
    if del_opt:
        del optimizer

    return pd.DataFrame.from_dict(results)

#---------------------------------


class View(nn.Module):
    def __init__(self, *shape):
        super(View, self).__init__()
        self.shape = shape
    def forward(self, input):
        return input.view(*self.shape) 
    
#---------------------------------
